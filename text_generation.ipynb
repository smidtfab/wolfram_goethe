{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t09eeeR5prIJ"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "GCCk8_dHpuNf"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovpZyIhNIgoq"
   },
   "source": [
    "# Text generation with an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hcD2nPQvPOFM"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/text_generation\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BwpJ5IffzRG6"
   },
   "source": [
    "This tutorial demonstrates how to generate text using a character-based RNN. We will work with a dataset of Shakespeare's writing from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Given a sequence of characters from this data (\"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly.\n",
    "\n",
    "Note: Enable GPU acceleration to execute this notebook faster. In Colab: *Runtime > Change runtime type > Hardware acclerator > GPU*. If running locally make sure TensorFlow version >= 1.11.\n",
    "\n",
    "This tutorial includes runnable code implemented using [tf.keras](https://www.tensorflow.org/programmers_guide/keras) and [eager execution](https://www.tensorflow.org/programmers_guide/eager). The following is sample output when the model in this tutorial trained for 30 epochs, and started with the string \"Q\":\n",
    "\n",
    "<pre>\n",
    "QUEENE:\n",
    "I had thought thou hadst a Roman; for the oracle,\n",
    "Thus by All bids the man against the word,\n",
    "Which are so weak of care, by old care done;\n",
    "Your children were in your holy love,\n",
    "And the precipitation through the bleeding throne.\n",
    "\n",
    "BISHOP OF ELY:\n",
    "Marry, and will, my lord, to weep in such a one were prettiest;\n",
    "Yet now I was adopted heir\n",
    "Of the world's lamentable day,\n",
    "To watch the next way with his father with his face?\n",
    "\n",
    "ESCALUS:\n",
    "The cause why then we are all resolved more sons.\n",
    "\n",
    "VOLUMNIA:\n",
    "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
    "And love and pale as any will to that word.\n",
    "\n",
    "QUEEN ELIZABETH:\n",
    "But how long have I heard the soul for this world,\n",
    "And show his hands of life be proved to stand.\n",
    "\n",
    "PETRUCHIO:\n",
    "I say he look'd on, if I must be content\n",
    "To stay him from the fatal of our country's bliss.\n",
    "His lordship pluck'd from this sentence then for prey,\n",
    "And then let us twain, being the moon,\n",
    "were she such a case as fills m\n",
    "</pre>\n",
    "\n",
    "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:\n",
    "\n",
    "* The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
    "\n",
    "* The structure of the output resembles a play—blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
    "\n",
    "* As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "### Download the Shakespeare dataset\n",
    "\n",
    "Change the following line to run this code on your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pD_55cOxLkAb"
   },
   "outputs": [],
   "source": [
    "# path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "path_to_file = \"korpus/merged_goethe_korpus.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### Read the data\n",
    "\n",
    "First, look in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aavnuByVymwK"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Length of text: 1052418 characters\n"
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Duhg9NrUymwO"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Faust: Der Tragödie erster Teil\n\n  Johann Wolfgang von Goethe\n\n\n  Zueignung.\n\n  Ihr naht euch wieder, schwankende Gestalten,\n  Die früh sich einst dem trüben Blick gezeigt.\n  Versuch ich wohl, euch diesmal festzuhalten?\n  Fühl ich mein He\n"
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlCgQBRVymwR"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "84 unique characters\n"
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## Process the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### Vectorize the text\n",
    "\n",
    "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IalZLbvOzf-F"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZfqhkYCymwX"
   },
   "source": [
    "Now we have an integer representation for each character. Notice that we mapped the character as indexes from 0 to `len(unique)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FYyNlCNXymwY"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{\n  '\\n':   0,\n  '\\r':   1,\n  ' ' :   2,\n  '!' :   3,\n  '\"' :   4,\n  \"'\" :   5,\n  '(' :   6,\n  ')' :   7,\n  '*' :   8,\n  '+' :   9,\n  ',' :  10,\n  '-' :  11,\n  '.' :  12,\n  '/' :  13,\n  '1' :  14,\n  '2' :  15,\n  '3' :  16,\n  '4' :  17,\n  '5' :  18,\n  '6' :  19,\n  ...\n}\n"
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l1VKcQHcymwb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "'  Faust: Der ' ---- characters mapped to int ---- > [ 2  2 29 51 71 69 70 20  2 27 55 68  2]\n"
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbmsf23Bymwe"
   },
   "source": [
    "### The prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wssHQ1oGymwe"
   },
   "source": [
    "Given a character, or a sequence of characters, what is the most probable next character? This is the task we're training the model to perform. The input to the model will be a sequence of characters, and we train the model to predict the output—the following character at each time step.\n",
    "\n",
    "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "### Create training examples and targets\n",
    "\n",
    "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
    "\n",
    "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
    "\n",
    "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
    "\n",
    "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UHJDA39zf-O"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n \nF\na\nu\n"
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ZSYAcQV8OGP"
   },
   "source": [
    "The `batch` method lets us easily convert these individual characters to sequences of the desired size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4hkDU3i7ozi"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "'  Faust: Der Tragödie erster Teil\\r\\n\\r\\n  Johann Wolfgang von Goethe\\r\\n\\r\\n\\r\\n  Zueignung.\\r\\n\\r\\n  Ihr naht euc'\n'h wieder, schwankende Gestalten,\\r\\n  Die früh sich einst dem trüben Blick gezeigt.\\r\\n  Versuch ich wohl'\n', euch diesmal festzuhalten?\\r\\n  Fühl ich mein Herz noch jenem Wahn geneigt?\\r\\n  Ihr drängt euch zu!  n'\n'un gut, so mögt ihr walten,\\r\\n  Wie ihr aus Dunst und Nebel um mich steigt;\\r\\n  Mein Busen fühlt sich j'\n'ugendlich erschüttert\\r\\n  Vom Zauberhauch, der euren Zug umwittert.\\r\\n\\r\\n  Ihr bringt mit euch die Bilde'\n"
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UbLcIPBj_mWZ"
   },
   "source": [
    "For each sequence, duplicate and shift it to form the input and target text by using the `map` method to apply a simple function to each batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiCopyGZymwi"
   },
   "source": [
    "Print the first examples input and target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNbw-iR0ymwj"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input data:  '  Faust: Der Tragödie erster Teil\\r\\n\\r\\n  Johann Wolfgang von Goethe\\r\\n\\r\\n\\r\\n  Zueignung.\\r\\n\\r\\n  Ihr naht eu'\nTarget data: ' Faust: Der Tragödie erster Teil\\r\\n\\r\\n  Johann Wolfgang von Goethe\\r\\n\\r\\n\\r\\n  Zueignung.\\r\\n\\r\\n  Ihr naht euc'\n"
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_33OHL3b84i0"
   },
   "source": [
    "Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"F\" and trys to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the `RNN` considers the previous step context in addition to the current input character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0eBu9WZG84i0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Step    0\n  input: 2 (' ')\n  expected output: 2 (' ')\nStep    1\n  input: 2 (' ')\n  expected output: 29 ('F')\nStep    2\n  input: 29 ('F')\n  expected output: 51 ('a')\nStep    3\n  input: 51 ('a')\n  expected output: 71 ('u')\nStep    4\n  input: 71 ('u')\n  expected output: 69 ('s')\n"
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### Create training batches\n",
    "\n",
    "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2pGotuNzf-S"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8gPwEjRzf-Z"
   },
   "source": [
    "Use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
    "\n",
    "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
    "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use a LSTM layer here.)\n",
    "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtCrdfzEI2N0"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwsrpOik5zhv"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RkA5upJIJ7W7"
   },
   "source": [
    "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
    "\n",
    "![A drawing of the data passing through the model](images/text_generation_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gKbfm04amhXk"
   },
   "source": [
    "Please note that we choose to Keras sequential model here since all the layers in the model only have single input and produce single output. In case you want to retrieve and reuse the states from stateful RNN layer, you might want to build your model with Keras functional API or model subclassing. Please check [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "## Try the model\n",
    "\n",
    "Now run the model to see that it behaves as expected.\n",
    "\n",
    "First check the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-_70kKAPrPU"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(64, 100, 84) # (batch_size, sequence_length, vocab_size)\n"
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6NzLBi4VM4o"
   },
   "source": [
    "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPGmAAXmVLGC"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (64, None, 256)           21504     \n_________________________________________________________________\ngru (GRU)                    (64, None, 1024)          3938304   \n_________________________________________________________________\ndense (Dense)                (64, None, 84)            86100     \n=================================================================\nTotal params: 4,045,908\nTrainable params: 4,045,908\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwv0gEkURfx1"
   },
   "source": [
    "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
    "\n",
    "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QM1Vbxs_URw5"
   },
   "source": [
    "This gives us, at each timestep, a prediction of the next character index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YqFMUQc_UFgM"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([31, 23, 31, 56, 76, 83, 65, 76, 29, 77, 59, 63,  8, 75, 33, 54,  8,\n       39,  2, 69, 48, 69, 56, 82, 42, 11, 78, 56, 16,  0, 65, 58, 46, 18,\n       81, 56,  6,  9,  1, 49,  9, 12, 57, 63, 26,  8, 68, 65, 20, 34, 57,\n       51, 67, 54, 59, 46, 44, 24, 22, 71, 79, 10, 11, 47, 38,  6, 66, 36,\n        4, 74, 35, 32, 56,  1, 63, 75, 40, 51, 21, 48, 52, 79, 66, 71, 41,\n       15, 18,  9, 19,  8, 30, 18, 65,  2, 10, 64, 39, 57, 13, 76])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfLtsP3mUhCG"
   },
   "source": [
    "Decode these to see the text predicted by this untrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWcFwPwLSo05"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input: \n 'ter deutscher Mann mag keinen Franzen leiden,\\r\\n  Doch ihre Weine trinkt er gern.\\r\\n\\r\\n  SIEBEL (indem '\n\nNext Char Predictions: \n 'H?HfzüozFÄim*yJd*P sYsföS-Öf3\\nohW5äf(+\\rZ+.gmC*ro:KgaqdiWUA=uÜ,-XO(pM\"xLIf\\rmyQa;YbÜpuR25+6*G5o ,nPg/z'\n"
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCbHQHiaa4Ic"
   },
   "source": [
    "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trpqTWyvk0nr"
   },
   "source": [
    "### Attach an optimizer, and a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UAjbjY03eiQ4"
   },
   "source": [
    "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
    "\n",
    "Because our model returns logits, we need to set the `from_logits` flag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HrXTACTdzY-"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Prediction shape:  (64, 100, 84)  # (batch_size, sequence_length, vocab_size)\nscalar_loss:       4.430278\n"
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeOXriLcymww"
   },
   "source": [
    "Configure the training procedure using the `tf.keras.Model.compile` method. We'll use `tf.keras.optimizers.Adam` with default arguments and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieSJdchZggUj"
   },
   "source": [
    "### Configure checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6XBUUavgF56"
   },
   "source": [
    "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "### Execute the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxdOA-rgyGvs"
   },
   "source": [
    "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yGBE2zxMMHs"
   },
   "outputs": [],
   "source": [
    "EPOCHS=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UK-hmKjYVoll"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/30\n162/162 [==============================] - 149s 919ms/step - loss: 2.5351\nEpoch 2/30\n162/162 [==============================] - 150s 926ms/step - loss: 1.8167\nEpoch 3/30\n162/162 [==============================] - 145s 897ms/step - loss: 1.6281\nEpoch 4/30\n162/162 [==============================] - 140s 862ms/step - loss: 1.4936\nEpoch 5/30\n162/162 [==============================] - 140s 865ms/step - loss: 1.3960\nEpoch 6/30\n162/162 [==============================] - 138s 853ms/step - loss: 1.3191\nEpoch 7/30\n162/162 [==============================] - 138s 850ms/step - loss: 1.2533\nEpoch 8/30\n162/162 [==============================] - 138s 849ms/step - loss: 1.1890\nEpoch 9/30\n162/162 [==============================] - 136s 842ms/step - loss: 1.1272\nEpoch 10/30\n162/162 [==============================] - 136s 842ms/step - loss: 1.0601\nEpoch 11/30\n162/162 [==============================] - 136s 841ms/step - loss: 0.9895\nEpoch 12/30\n162/162 [==============================] - 136s 842ms/step - loss: 0.9124\nEpoch 13/30\n162/162 [==============================] - 136s 842ms/step - loss: 0.8337\nEpoch 14/30\n162/162 [==============================] - 137s 844ms/step - loss: 0.7530\nEpoch 15/30\n162/162 [==============================] - 136s 842ms/step - loss: 0.6767\nEpoch 16/30\n162/162 [==============================] - 137s 843ms/step - loss: 0.6093\nEpoch 17/30\n162/162 [==============================] - 137s 844ms/step - loss: 0.5517\nEpoch 18/30\n162/162 [==============================] - 138s 850ms/step - loss: 0.5023\nEpoch 19/30\n162/162 [==============================] - 137s 844ms/step - loss: 0.4636\nEpoch 20/30\n162/162 [==============================] - 138s 850ms/step - loss: 0.4353\nEpoch 21/30\n162/162 [==============================] - 141s 871ms/step - loss: 0.4126\nEpoch 22/30\n162/162 [==============================] - 139s 857ms/step - loss: 0.3926\nEpoch 23/30\n162/162 [==============================] - 140s 862ms/step - loss: 0.3762\nEpoch 24/30\n162/162 [==============================] - 140s 862ms/step - loss: 0.3666\nEpoch 25/30\n162/162 [==============================] - 140s 865ms/step - loss: 0.3574\nEpoch 26/30\n162/162 [==============================] - 136s 840ms/step - loss: 0.3517\nEpoch 27/30\n162/162 [==============================] - 136s 838ms/step - loss: 0.3435\nEpoch 28/30\n162/162 [==============================] - 136s 839ms/step - loss: 0.3408\nEpoch 29/30\n162/162 [==============================] - 136s 838ms/step - loss: 0.3379\nEpoch 30/30\n162/162 [==============================] - 136s 839ms/step - loss: 0.3331\n"
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIPcXllKjkdr"
   },
   "source": [
    "### Restore the latest checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LyeYRiuVjodY"
   },
   "source": [
    "To keep this prediction step simple, use a batch size of 1.\n",
    "\n",
    "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
    "\n",
    "To run the model with a different `batch_size`, we need to rebuild the model and restore the weights from the checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zk2WJ2-XjkGz"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'./training_checkpoints/ckpt_30'"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LycQ-ot_jjyu"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71xa6jnYVrAN"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (1, None, 256)            21504     \n_________________________________________________________________\ngru_1 (GRU)                  (1, None, 1024)           3938304   \n_________________________________________________________________\ndense_1 (Dense)              (1, None, 84)             86100     \n=================================================================\nTotal params: 4,045,908\nTrainable params: 4,045,908\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "### The prediction loop\n",
    "\n",
    "The following code block generates the text:\n",
    "\n",
    "* It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
    "\n",
    "* Get the prediction distribution of the next character using the start string and the RNN state.\n",
    "\n",
    "* Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
    "\n",
    "* The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one character. After predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters.\n",
    "\n",
    "\n",
    "![To generate text the model's output is fed back to the input](images/text_generation_sampling.png)\n",
    "\n",
    "Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 75000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " Ihm wird es, flumpf nicht, wilden steige schrecken.\n\n\n  SCHÜLER:\n  Auch um Lüftchen leidlich schwer,\n  So sei die Flatternde, die Flüchtige faßt.\n  Es ist, die Lühne schürzend Kind\n  In dir verehrend, viel zuvor:\n  Die lieblichste Tochter brennt von Stahl gewognen!\n  Gar soll sich in der Welt zerstreuet.\n\n  Und lügste rückt und eilt;\n  Wenn am nun steht.\n  Am besten ist's auch sei,\n  Wird jung entführt, in diesen Höhlen, wie's an Wein.\n  [In diesem Kuhr süße;\n  Die die Wildenflören auf dem bergigen Wies!\n  Und hättest du in Ohnmacht.)\n\n\n\n  Walpurgisnacht\n\n  Harzgebirg aus Löten, glaubt ihr, seitwärts der Mitzerstreit  MEPHISTOPHELES:\n  Bedenklich ist es, aber hör' ich doch das Herz, die Welt zerstreuet.\n\n  RUht!  nur nach allen hier verhaßtet,\n  Ist in der Kätze Luft,\n  Und so entlass' und Schranken heimlichen Dort zum Orkus mich\n  Gewaltigen Pudel;\n  Es schlepp' ich wie der Diener bei!\n  Die Zwecke schauend um uns hinaus.\n\n  NEREIDEN UND TRITONEN:\n  Töpproppelen.\n  Stumpfend, zuwer Boden,\n  Fürchte mich was durfgegeben\n  Undig Brust im Meer,\n  Das bringt mir bösen Ruf uns fluche nun entzünden,\n  Ein Feuer, leuchtend, blinkend, platzend,\n  Wie man die Welt\n  Im Reicht Gestand\n  Zu Funst, sich deines Volksgewicht,\n  Der über die Natur und ihre heil'gen Wer sich's nun mein Stünde\n  Künftig unbesitzt,\n  Den Weg verfolgen über der Erde Preis.\n  Von hier aus--+\n\n  FAUST:\n  Du nennst mit den Tales, ein Irrlichten\n  Zweige hebt, leider!\n  Halbtaunt ein neues Chor?\n\n  FAUST:\n  Ich war nun ist's ein Gaukeln streifen,\n  Nach einer fort: zu meiden,\n  Nach soll sie würden sie nimmer müde, wo die Zitterwellen,\n  Ufer aber fluch erschuldenfrei.\n\n  EIN ANDRE:\n  MEPHISTOPHELES:\n  Verlangsam, absurd in jedem Laut\n  Den freien Geist, der das RISTOPHELES:\n  Vor allem aber schützen\n  Liegenduft!\n\n  SIEBEL:\n  Setzte Schall nicht viel damit getan.\n  Was sich der Flammen aufgeregt;\n  Man liebt's, ist es Fels- und Schmeichelkätzchen;\n  Warum stand sie allein.\n\n  KNABE LENKER:\n  Hier steht ein Mann!  da, fragt der Gnaden.\n  Wie scharf der Teufel bin.\n  Nun, habt ihr nicht sogleich begehrt,\n  Persöhnen\n  Dem könnten darf' ich mich doch wenig kümmern;\n  Schlägt ein Paris von deinem Schlag;\n  Wo die das Ortzen sitzt.\n  Dergleichen hab' ich nie ein Angesicht.--\n  Mir sprüht er heißen,\n  Er lieft der Reer blitzt komm, und kennt!\n\n  MEPHISTOPHELES:\n  Ich rühre nicht.\n\n  FAUST:\n  Murch die Majestät\n  Der Geist nicht vom Wie hält's durch die Lippen, die Triumph fernen Raum gewundnichts!  Sie wird bei einer Nacht!\n  Das oben Fraung im Säubel voller Saft,\n  Laß das Vergang in den Kufber Mühen\n  Ger  Und wir finden):\n  Er ist, mit leichter Süßen einst gestoppelnd an,\n  Doch war die Wald, das wird sich alles geben;\n  Sobandeln;\n  Dort wirket sie geheime Schmerzen,\n  Unrängeln tanzt man, wen ich Fühle nacht.\n\n  HABEBALD:\n  Kein Drei und Frieden--\n  Läßt nicht, wenn er gleich, kain Handeln das zunächst\n  Doch darf mich's nicht verdraußen?\n\n  FAUST:\n  Schreckliches Gesäufte\n  Die Kunst ist alles schön,\n  Da hielten aach deine Lippe regt;\n  Ins unbegreiflich holde Kraft von Ilios\n  Umlagerts, auf uns Geister,\n  Wie ich ihn nun ihr Menicht aufzurühmen Tönen.\n  Ich habe nicht der du, so wundersch.  jeder worgen Mund\n  Nicht ungeduldig Vater traget dich immer, was du schwebst mich viel verheißt,\n  Und doch den Gauch, der du mir zu ersannten,\n  Ein jeder sucht sich einen Blick,\n  Das beliebzen sich behagen,\n  Ein Berg, zwar unvöllig drüben wir,\n  Verfahret nur nicht, es sind ihrer Tod!\n\n  FAUST (liegt sie eine Pfirsicher,\n  Auf ungemäßig Wohnung magst entfüllt,\n  Das Ungesetz gesetzlich überwalten\n  Porsch da drinne: Hellung und Ckorne,\n  Bald gesteh' ich Auge schloß ich in der rechten Spur;\n  Doch weil es Eisen kann's mit Freuden und Gewalt,\n  Daß ich entfernend, die ich heilig weiche.\n  Doch euresgleichen hab' ich nie\n  In heiter kommen, heiter zugesellt,\n  Doch ihrepfennt ein König mir sendenden Aage,\n  Der als ihr glücklich schlangen\n  Sakt dir die Schale knabene,\n  Oberon, den samen?\n\n  FAuendigstem Besicher\n  Aus eurem Arm von Nebel sehn!\n\n D\n  Da sah ich, bei dem hohen FAUST:\n  Und du!  Was hat dich heute\n  Den schaten Da Sproch, warum man wachte ein solchem weinennen\n  Bescheiden gebt ihr weiter geht,\n  Was wählt Ihr niemals noch erhält.\n\n  ECHO:\n  Stürzend gühnen,\n  Nein, er gefallen Wegne-DaKeinen Himmelswand umzugehen,\n  Mag alles wunderliche Frucht,\n  Und Hoffnung wie davon.\n\n  DIE GEISTER:\n  Wie werden an der Erde lebt,\n  So kann denen Schelm vielleicht!--der Unsinn vor einem Weite!\n  Soll man Menschenstimmen haben.\n\n  MARGARETE:\n  Ich war breibt;\n  Das Kramiden schlecht.\n\n  KAISER:\n  Der Körper liegt, und was ich tun.\n\n  FAUST:\n  Das ist die Magd!  das ist alles vom Feuer umronnen;\n  So herrsche denn, wer dir allen,\n  überfüllten, ewig reiner Erde jange\n  Einsiedlich auch nicht fertig.\n\n  FAUST:\n  Nein, gleich sollst du versammelt,\n  In Fels- und Höhler, war das leichte Weiten,\n  Geleit ihn tapfer und genugt?\n  Wer schlagt und sat, als wogt in stillsten Schatz gesetzt--\n  Unaus, hält sie sich vom schönsten Frauen\n  Verklärte von dir wende!\n  Die Menschen zu besehn,\n  Da findest du zu jeder Klufen Mare flochtet,\n  So ist ein kleiner Raum ihr nun im Glut der Nachreut hinweg.\n  Ich wenn es wohl, daß mich der Herr nur schont hat Garten'\n  Und von der Apfe stand verlassen dir davon nichts abgezwackt.\n\n  FAUST:\n  Nur muß man sichen rhn untersvor!\n  Und die Koste drängen sich und ihre großen Falle\n  Über die halbe Höllenbrut\n  Ist nur ein Gleichnis;\n  Das Dunstige schönes Gut verdauen?\n\n  FAUST:\n  Ist jedes Blättchen gut.\n  Du bist exendliche\n  Zwar reizendes Gesicht\n  Berg- und Feuer,\n  Mir tätig selbst sie doch herbei.\n  Und so wie die Gulden war.\n  \"Mein Frecheschätze,\n  Zum Garten preist den Rücken.\n\n  RINTE:\n  Die Menge drängt heran, Euch zu umschranzen.\n  Ich bitte dich, laß mich allein\n  Und überlegt ihr denn, daß ihr des Hauses Schar\n  Und werfen in den Spiegel doppelt eine Schilferheit zu mich element.\n\n  MEPHISTOPHELES (zu Faust):\n  Her MEPHISTOPHELES:\n  Das sprech Vorspiel rafft.\n\n  SIEBEL:\n  Betrug war alles, Lugerdiestreites Schatzt.\n\n  HOMUNCULUS:\n  Lebendühmen hier das Herz zur Hand.\n  Was hör' ich wohl, denn keiner verkündet's, daß es im Nächte wiederfreuen!\n  Indessen wir die halbe Welt gefällt,\n  Der mir so kann,\n  Gesamt zu Tufen, der davon.\n\n  FAUST:\n  Ich wußte nie einen wilden Rasen!\n  Ein mäßig angenehmstag--\n  Und wie ihr auch verdient ein seltsamst Geld und Schönheit zuhen,\n  Der Schäfertraulend groß und klein.\n  Ein Gaukel reißen auf mit der Besitzt vergessen,\n  Sie gehn dein Lost, wenn's keine Stunde sie die Pfropfen ging,\n  Das paßt dir da, zu Tun zerstört\n  Und Tritt und Kn.\n Stimme nicht die höchste Zöpuchten\n  Und zeig uns gleich die Zeit bewegt!\n\n  FAUST:\n  Ich weiß nicht, soll's ihr übel gehn,\n  Das hat der ewige Gesang.\n\n  WATE:\n  A!  war die rasche Kreis.\n  So sehr mich's wohl, so sehr es in der Nähe\n  Wie sie dein Völker an Faust und Haupt und Brust.\n  Du bist und bedacht's und paßt;\n  Für alle Frager schon,\n  Als über Bin und Echt,\n  Ich liebe mir den Fels wir da\n  Zur Feuerquelle sacht heram mit giftig klaren;\n  Schon es doch nicht aus, der Spaten reicht,\n  Ist jedoch eine lassen sollten,\n  Es war sonsch und viel Gewinnen\n  In liebliches Ferde;\n  Denn er hat sich sein behender Lande\n  In uns Geneigeschlagen\n  Zwar Perojen auf und ab geheimnisvoll verbirgt.\n  Doch Erde Mühe will drinnen, da erlagen\n  Und Ehwenbraut gar endlich regt.\n  Sie werfen sich aus der Ruhm zu Flauße;\n  Fall hat es wärm gemacht:\n  Wie sie dir nicht.\n  Der Philyro  MARGARETE:\n  Ich will zu meiner Zeit schweifen,\n  Und alle Rechte, kräftig.\n  Die Menschen da zum Unerfiehlust\n  Seh' ich, wie er sich gleich das Wissenspyzieren,\n  Was jener Süße,\n  Das an, die Himmels Meer gefühlt zur Quell sich's nur gewärtig,\n  Und da, wo ich bleiben darf.\n  Ein Widerschein der Drachen dir erfüllen?\n  Nur fest am Chem  ist es darf.\n  Müßt ihr nicht bin.\n\n  FAUST:\n  Ist über vierzehn Jahr doch alt.\n\n  MEPHISTOPHELES:\n  Nein!  Sie wird bei einer Nachther ging;\n  Über die sage Die erschafft\n  Und zeig uns glitzert's um, ich spüre nicht der Boden bläht...\n  Nur rings Weibhabichen\n  Drum mit Kraft umwitzen sollten,\n  Es sage:\n  In die Ewigkeiten wird zum Eigenschaftraft,\n  Laßt mich hören, daß der Kopf ihm fehlt.\n  Jedoch zum höchsten Prachtgespenster überstehen!\n\n  MEPHISTOPHELES:\n  Denn man wird es mir das lichte Nacht beschimpfen!\n  Soll wie ein böser Haut der erste Strahl der Frauen.\n  Nun ort bei Nachbariannt,\n  Glaubhäftigen.\n\n\n\n  Helankelstunde\n  Zu preisen Gott den Herrn.--\n  Hab' ich Unterwellen dann zuleuchtet seitschwärmten Schnieden,\n  Von der Mauerhöhle ein Andacht fang' ich an\n ortigster Gewalt,\n  Was andre tätig kommen es darum mit Mund.\n\n  BLUTUS:\n  Noch braucht es mächtig.\n  Die Berge, die Stadten stehn\n  Die Königin, sie blickt gelassen.\n  Keine Wälle, keine Mauern,\n  Jeder nur sich selbst ums Welt umfassen,\n  Mit einem Worte knarr' ich dich.\n\n  MEPHISTOPHELES:\n  Original, für die Dirne schaffen!\n  Auch hab der Mann die Ratke\n  Was überlaubt der Kind daran\n  Und warfeng Heile,\n  Das hatzt uns heute sein.\n\n  MEPHISTOPHELES:\n  Ich weiß nicht recht, wie muntern?\n\n  MEPHISTOPHELES:\n  Das lassend sind wir nicht hm andre Frage.\n  Mit Erinn ungemeuer lehren.\n\n  FAUST:\n  Welch ein Gesprnnden, wie du leicht.\n\n  FAUST:\n  Ich wache jener Welt\n  Gemächlichen Augen lagen,\n  Wo man die Menschen zu bessern und zu Versätzen,\n  Bald lieb mich heiß;\n  Nicht mit!  Ho! durch Proteus!  Das ist die Brust,\n  Die eine wie der Kinsche harren,\n  Es sei die Zeit in Frechein saufrer, Kreise!\n  Schon in der innerschnellen Leibe\n  In Wundermann sattraut.\n  Da mag sie denn sie waschen war,\n  Fast kühne Ruh ist klar!\n\n  SPINXE:\n  pur Netterie Königin dabei,\n  Doch alle überbalschlichten,\n  Von einem Wunder die Welt erst gar nicht bange.\n  Nach der Straßen mit Gewalt\n  Von der ungeheurem Schweins\n  Entdeck' ich mepfen,\n  Wird der kluge Finger schlichten.\n\n  ERZKANZLER:\n  Dem Purpur der geschwind regel schmiegen,\n  Die Kiste bis zum Dienst getreune.\n\n  OBERON:\n  Reihenwaft der Dinge.\n  Erklunger!  Im stiller Gnaden!\n  Hier, Doktor, Des Phaoke,\n  Das ab?\n\n\n\n  Gretchens Stube.\n\n  Grab zu erfragen;\n  Soband wie naschweigen!\n\n  MEPHISTOPHELES:\n  Ich raffe keine Skunden wären\n  Die ich doch auch als letzte Stürme schön,\n  Doch schlag an diesem Erdenkreis Vergnügen,\n  Die Rimpel wirke trägt's alle Jung\n  Eurer Schwaner,\n  Der Sist Gesucht Maskeradenspiel,\n  Mag jeder Scham versagt dir schwer,\n  Schon blutig blinkend kühn\n  Und eure Schwäger,\n  Der mit Nebelsfreuden zuverend;\n  Miß ist ein jedes Paar,\n  Es findet statt Gewalt man sich, mit allen ist n;\n  Wer ruft der Freund, verbärme dich, und was ich tun.\n\n  FAUST:\n  Nun kenn ich deine Rechte schlug!  Wir könnten uns wiedersehren.\n\n  FAUST:\n  Seid uns behend von ihm entfernend;\n  Geschrei zuletzt.\n\n  MEPHISTOPHELES:\n  Ich weiß nicht mehr,\n  Und STad und Bild bewegt davon.\n\n\n\n  Am EN:\n  Curicht er im Herzen Wort\n  Verquällt die Frechheit, Moder nein!\n  Was willst du at das Ohr getan,\n  Daß ihr die Flatternde, die Flüchtige Raum und Gruß\n  Hier trotzten Rom und Griechenlosen, wie es ballt,\n  Das ist die Kunst ist lange nicht scheiden;\n  Das Warpationen,\n  Viel Ist die Bein, vergiesen Magen,\n  Wie schön Gebild, umwebt sein Bergsamko mir näher;\n  Schon entsteht ein Geist find' ich mir an deinem Herzen streitet--\n  Ach!  kaist es nicht viel zu sagen,\n  In diesen Stecht jede Klauen.\n  O gibt's ein Scharlate wer.\n\n  FAUST:\n  Muß ich denn gehn?  Lebendige Philosophon bin ich nicht.\n\n  MANTO:\n  Des Fadent Schulden, weil dir auf Eurer Schwelle.\n  Mitten in ägypten auch.\n\n  HELENA:\n  Verlassen habt doch Masken;\n  Aber wir?  +\n\n  PHORKYAS:\n  Königin, du bist gemeint!\n\n  HELENA:\n  Ich, aber höchste Spfang\n  Lassend ruhig ist dir einzig überschafft,\n  Das hohe Werk zum Sitze,\n  In Waffen blickst sie an das Beste deiner Kühle drennen.\n\n  Er drüben wir uns wir wolltest, es sei vor aussurdeuten rechter Qualen.\n  Dächts wird feucht erfahren,\n  Dank die Herz vom Meer lügen,\n  So setzet sich der große Hauf,\n  Herr Urian sitzt die Bestialiamme Verbraut und Geil verwerflichen,\n  Dann darf's und in die Kreuz darf's nicht geraten noch den innern Sache,\n  Der Entschlug ist alles übermüte\n  Daß die Natur des Fasse mit herangewacht:\n  Daneben braucht man dir.\n  Doch was er wirkt und laß ihr ein Volkstut weiter fühlen.\n\n  MEPHISTOPHELES:\n  Man denkt an das, was man verliert;\n  Alle doch ich schön!  Du gönnst mir ja mein Glück;\n  Doch lisplich, wie sie sind.\n  (Gehn vorüber.))\n\n  WAGNER:\n  Es ste gezugenessen war,\n  Das ist kein kurz, das Schöne wieder n Lüften\n  Altan durch das war ein harter Tritt, +\n  Der, neige.\n  Das Glöcklein läutet, und ich was zum besten.\n\n  SIEBEL:\n  Was steigt aus dem Boden her?--\n  Natürlich, gescheiter ach!\n\n  SCHÜLER:\n  Ich bin ein armer Kellernesprungen,\n  Allein erfalltüsiert;\n  Das Wappen weif nicht aus.\n  Du sollst dahin wüsten wir zu fliegen,\n  Als Dienst du ab!\n\n  OPHELES:\n  Und wie ich diese Feuerchen du dir, von der Kappe\n  Aus dem die Sagische Leben erhöht.\n\n  SIRENEN:\n  Ein Giebeln unter denkt, und Wechselnd zaudern,\n  Durch das Moos und durch die Hügel, dort im Kreis, erhebt sich an,\n  Das Was nur gewinnig.\n  DUm Schmause, diese Buhres h fort):\n  Die Feuer flammend nie gewährt.\n  Was soll das lüttern wir der Ilich werden konnte,\n  So lieb' ich dich, und was ihr seid Ihr wohlbekannt;\n  Doch lehren uns heute nacht.\n\n  HALLE:\n  Der alte Nicht der tritt hervor!\n  Nebel zu!  der Geist erwidern.\n  Weiß nicht, o holde dich länger fremden Pfaden.\n\n  FAUST:\n  Ja, wenn der Pfarrer Seelen wächsten Arm erhaschen Magel kein Wechselstreit\n  Die Kissen unsres Heeres, der das Wort der Lager hin.\n  Nie andre hat sie je gleicherweiternde, im Tiefsten Winmernächte.\n  Wahrhaftig laut der Schwestern und läßt sich t;\n  In dieser Kunst möcht ich was profitieren,\n  Doch tritt nur das Meer davon.\n\n  DAME:\n  Ist doch ein jedes Bild?\n\n  EILEBEUTE:\n  O!  welch ein Garten Iormart dir das geschehn?\n  Was hilft's, daß man die Welt verschoben?\n  Die Majestät vergießt noch mehr:\n  \"Ich bin leben,\n  Es sind dir an diesem Pulle hn nichts an dieser Welt gelange,\n  So gefällt mir daß als andre weid ich wohl davon;\n  Frisch an und das hier heimlich an\n  Bestätigung, Letter,\n  Hat sich in diesem Zauberspiegel!  das sind Gebär' ich den Haaren.\n  An Hoffnung reiche Sammlich Wesen treibt.\n\n  RAUFEBOLD:\n  Welche dies Land beglückt.\n\n  FAUST:\n  Ich stellt's Meeres, das ist geben.\n  So elle Welt erstaunt.\n  So wird er statt vor den Augen des Volkesippin, Helios Entsetzen\n  Drängt die hast du's machen!\n  Ein jeder seht ein menschenähmlich Reihe,\n  Und seine ruh,\n  Du bleibst zu Taten,\n  Geflüster, der das Wort zu Tage Flammen spend' er die Tür, herauschen dir.\n\n  MEPHISTOPHELES (leise zu ihr):\n  Ich kann von diesem Busch gestopft-\n  Und heutz, mit dichträltigen Hunger dort, wir wird zur Sinne drängen,\n  Zur Nachbarschaft der Unterwelt!\n  Wer sind die Menge durch die Flur;\n  Die Maut das unterste Laufe!\n  Wie steht es doch und uns, da weiß das Beste;\n  War ich das alles?  Bin ich's?  Werd' ich's müßt ihr euch einmal nicht bleiben!\n\n  MARTHE:\n  Ist tot.\n\n  ERSTER:\n  Ihr bricht mir das ich mir die Länge freuen?\n  Es ist ein klein Paris, und bildet steht sei mir nächstem Geschmerz.\n  Denn wer den Saal sich!--Kaum ihn zu spüren,\n  So feuchte Würfung wahr.\n\n  HELENA:\n  Mich doppeltand!  Ich hütte mich so lieb!\n\n\n  Zu allemporten\n  Erst im Funken wünscht in der Kanze die Wahrheit;\n  Daß sie vom Bösen\n  Froh sich erlangen;\n  Platz und Pflippen wandeln, man zu gefallen,\n  Dann bricht allein, das Leben sei's,\n  Das ist der Gottheit, denn in Natur verlänger.\n\n  MEPHISTOPHELES:\n  Ruh und äther zu!\n\n  FAUST:\n  Muß ich dem Menschengeist Verschwind ich mir seltenbraut,\n  Wo jedes Element!\n\n  CHORUS  ja aufgewälzt,\n  Zudeinsinnigem Klipps an der Erde\n  Sei's die heiterste Gebärde,\n  Die Sache sie anfaßt, alles hat ein Wandrer dich.\n\n  MEPHISTOPHELES:\n  Bescheidne Wahrheit sprech.\n\n  FAUST:\n  Ihr wäret wert, nun, was ihr schafft.\n\n  MEPHISTOPHELES:\n  Tur sie geleiht uns dazu.\n  Vor allen Häusern Sparuch haben wir uns nicht geraue:\n  Wie sich auf Schlaf im Reich uns deutlich, was der Mensch bedacht,\n  Was es zittern, drintend herrscht.\n  Durch Fenster, die auf Faustens Arm,\n  Solang er an dem Boden hin\n  Und deckt auf mich zusammenstürzen\n  Und spieller Säfe,\n  Lügtliebende,\n  Gebtanden,\n  Der darf den Augen tragen,\n  Was auf Hochgefihr der Nacht,\n  Die Wechselgesang.\n\n  BACCALAUREUS:\n  Das hab' ich wohlgefällt.\n\n  HELENA:\n  So hört uns ganz allein,\n  Für die Einsicht so warm nicht schmeckTOPHELES:\n  Ein glühnder Schmerz erleichtet umsieht, steigst du Phantast.\n  Ich sehe nichts-- und Schaufelnd nieder,\n  Ein Lumpen alte, grinses Dinge\n  Ein wahres durch die Ohre grenzenlos erschlagen,\n  Und fürs Kommando bleibt man auch pragt.\n\n  PHORKYAS:\n  Die bunten Tische.  Das war ich sonst bekommen.\n  Dies begehrt: Der Erde Graben.\n\n\n\n  Dorch Zeit ergeben,\n  Ob einer wächst durch Saat und Büschen werfen,\n  Wenn es nur länger dauerte.\n  Wo bin ich regel wachsende verbanden,\n  Da liegt der Spitne, Busen Geist,\n  Dann strömt sie nach, wohin der Freunden Schwerg!\n  Sie ache, welch ein Feuer\n  Und immer zierlich regt.\n  Sie werden keine Skurze nicht.\n  Komm' ich als Gattin?  komm' ich eine Königin.\n  Durch Weiberkünste, was auch drohend--\n  Erhasche dir!\n  Ein wenig rechts, der wird es angefangen?\n\n  ALTMAYER (zieht einen hochgetüm dabelasse bit ich mir nicht.\n  Schon sinkt es nicht.\n\n  GREIFE:\n  Herein, hinaus!  Das vermag ich.\n\n  FAUST:\n  Das ist die Brust von Herd und Haufen\n  Ein seliger schaden Treib.\n  Ihr seid ja d er fest an mich her einen besser arfallscht recht, so mag's wie ein Torab, den Trauben krauen\n  Solcher Wunder Lind!\n\n  GREIF:\n  Natürlich!  Die Vielgespanten\n  Sohn zu sein, auf der Stelle doch nicht sag uns an der Stille,\n  Drangen wir uns dir auf der Erde\n  Sei's das hier sie treten:\n  Ja, es tagt der Schaum sie nicht verkünd'gen\n  Sind gern im O MARIAN:\n  So groß es stets das Gute\n  schafft.\n\n  FAUST:\n  Was schlur sucht der Zukunft nur an Schlünde\n  Des Menschengeist Verstand uns dazu bewegest\n  Gewährt allein, das Lobenswürdige\n  Zu rühmen, wie zu sagen.\n\n  DRYAS:\n  Die Phantasie, in Läng' und Ende.\n  Und ohne Narrheit, jeder nur als erste Frau.\n  (meine Krieger hinzuläufen,\n  Sind Unbeht ist es nicht aus dem Hund,\n  Verfaulschte Welt, sich in der Spaß Trieben miß ten Anblick wohl bedacht,\n  Immer schwankende Gestalten,\n  Daß ich euch weniger freie, ruft mir leid.\n\n  MEPHISTOPHELES:\n  Weh!  ich erte, was sie wacht;\n  Zuletzt der tiefsten Hexen der Gewölb des Wangeneh schon\n  Von dieser häntlich zu setz hinaus?\n  Nach allem andern Müttern Sch dich immer satt;\n  Ich weiß dir so ein Herz ist uns verkünd'g\n  Seinen Blicken, seine Lauf.\n  Die Theingst sie doch begegnen ist,\n  Der heißen Fiebelnte!  Aus Schlafen Windes Reis sich nicht zu schwanken.\n\n  DIE TIERE:\n  Und was ihr wünscht, das wüßte was ganz baug und Kall, zum bösen Geist im Kreuz der Herrscher lebendigen Antan,\n  Uns heimlich tragen.\n\n  MEPHISTOPHELES:\n  Nennte was zu nennen,\n  Wer auf dieser Scherb von Wort zu Höchenlose Necherholte mir das Glück.\n\n  FAUST:\n  Nur bist du rechte jetzt in diesen Hauchst du so groß.\n  Ich denke wohl, ich mache selbst\n  Die Anschlammen das ungeduldig hin\n  Berufu wir auf Sand im Grenzen\n  Erschreckten Weiberschmach.\n\n  FROSCH:\n  Komman der Felse ballen sich.\n  (Sie steht es seinem Bandelntlüpflich auszuüber Gestalt\n  Heut über dich!\n\n  MEPHISTOPHELES:\n  Ich war besondre Wirkung spalt!\n  Tretet jedes Kinderspiel.\n\n  MEPHISTOPHELES:\n  Denkrag nicht gehört.\n  Die Frau ist grimmig,\n  In den wäre dich und durch die schönsten Sterne\n  Und von der Erde Druck,\n  Die Kette singt, zur lustiger Geisterschnaren.\n\n  FAUST:\n  Was schafft ihr hier ein paradiesisch Land,\n  Da rase draußen Flut bis auf zum Rand;\n  Seit dieser ernsten Scharen\n  Wie standen auf dem borgt der Feuers Weib hat tiefsinnen Hadiene \n"
    }
   ],
   "source": [
    "START_STRING = u\"MEPHISTOPHELES\"\n",
    "\n",
    "model_output = generate_text(model, start_string=START_STRING)\n",
    "\n",
    "filename = \"model_out.txt\"\n",
    "out_text = open(filename, \"w+\")\n",
    "print(f\"Generated text for {START_STRING}:\")\n",
    "print(model_output)\n",
    "out_text.write(model_output)\n",
    "out_text.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AM2Uma_-yVIq"
   },
   "source": [
    "The easiest thing you can do to improve the results it to train it for longer (try `EPOCHS=30`).\n",
    "\n",
    "You can also experiment with a different start string, or try adding another RNN layer to improve the model's accuracy, or adjusting the temperature parameter to generate more or less random predictions."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('NS': conda)",
   "language": "python",
   "name": "python38264bitnsconda5906bd2dc1be4fac85b3821c63f25579"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}